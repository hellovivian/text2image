{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50065109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally made by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings)\n",
    "# The original BigGAN+CLIP method was by https://twitter.com/advadnoun\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "# from email.policy import default\n",
    "from urllib.request import urlopen\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# pip install taming-transformers doesn't work with Gumbel, but does not yet work with coco etc\n",
    "# appending the path does work with Gumbel, but gives ModuleNotFoundError: No module named 'transformers' for coco etc\n",
    "sys.path.append('taming-transformers')\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models import cond_transformer, vqgan\n",
    "#import taming.modules \n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.cuda import get_device_properties\n",
    "torch.backends.cudnn.benchmark = False\t\t# NR: True is a bit faster, but can lead to OOM. False is more deterministic.\n",
    "#torch.use_deterministic_algorithms(True)\t# NR: grid_sampler_2d_backward_cuda does not have a deterministic implementation\n",
    "\n",
    "from torch_optimizer import DiffGrad, AdamP, RAdam\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import CenterCrop, Resize, ToTensor, Normalize\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import kornia.augmentation as K\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "from PIL import ImageFile, Image, PngImagePlugin, ImageChops\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "import re\n",
    "\n",
    "# Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Check for GPU and reduce the default image size if low VRAM\n",
    "default_image_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2fe0b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Various functions and classes\n",
    "def sinc(x):\n",
    "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
    "\n",
    "\n",
    "def lanczos(x, a):\n",
    "    cond = torch.logical_and(-a < x, x < a)\n",
    "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
    "    return out / out.sum()\n",
    "\n",
    "\n",
    "def ramp(ratio, width):\n",
    "    n = math.ceil(width / ratio + 1)\n",
    "    out = torch.empty([n])\n",
    "    cur = 0\n",
    "    for i in range(out.shape[0]):\n",
    "        out[i] = cur\n",
    "        cur += ratio\n",
    "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
    "\n",
    "\n",
    "# For zoom video\n",
    "def zoom_at(img, x, y, zoom):\n",
    "    w, h = img.size\n",
    "    zoom2 = zoom * 2\n",
    "    img = img.crop((x - w / zoom2, y - h / zoom2, \n",
    "                    x + w / zoom2, y + h / zoom2))\n",
    "    return img.resize((w, h), Image.LANCZOS)\n",
    "\n",
    "\n",
    "# NR: Testing with different intital images\n",
    "def random_noise_image(w,h):\n",
    "    random_image = Image.fromarray(np.random.randint(0,255,(w,h,3),dtype=np.dtype('uint8')))\n",
    "    return random_image\n",
    "\n",
    "\n",
    "# create initial gradient image\n",
    "def gradient_2d(start, stop, width, height, is_horizontal):\n",
    "    if is_horizontal:\n",
    "        return np.tile(np.linspace(start, stop, width), (height, 1))\n",
    "    else:\n",
    "        return np.tile(np.linspace(start, stop, height), (width, 1)).T\n",
    "\n",
    "\n",
    "def gradient_3d(width, height, start_list, stop_list, is_horizontal_list):\n",
    "    result = np.zeros((height, width, len(start_list)), dtype=float)\n",
    "\n",
    "    for i, (start, stop, is_horizontal) in enumerate(zip(start_list, stop_list, is_horizontal_list)):\n",
    "        result[:, :, i] = gradient_2d(start, stop, width, height, is_horizontal)\n",
    "\n",
    "    return result\n",
    "\n",
    "    \n",
    "def random_gradient_image(w,h):\n",
    "    array = gradient_3d(w, h, (0, 0, np.random.randint(0,255)), (np.random.randint(1,255), np.random.randint(2,255), np.random.randint(3,128)), (True, False, False))\n",
    "    random_image = Image.fromarray(np.uint8(array))\n",
    "    return random_image\n",
    "\n",
    "\n",
    "# Used in older MakeCutouts\n",
    "def resample(input, size, align_corners=True):\n",
    "    n, c, h, w = input.shape\n",
    "    dh, dw = size\n",
    "\n",
    "    input = input.view([n * c, 1, h, w])\n",
    "\n",
    "    if dh < h:\n",
    "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
    "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
    "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
    "\n",
    "    if dw < w:\n",
    "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
    "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
    "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
    "\n",
    "    input = input.view([n, c, h, w])\n",
    "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
    "\n",
    "\n",
    "class ReplaceGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x_forward, x_backward):\n",
    "        ctx.shape = x_backward.shape\n",
    "        return x_forward\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        return None, grad_in.sum_to_size(ctx.shape)\n",
    "\n",
    "replace_grad = ReplaceGrad.apply\n",
    "\n",
    "\n",
    "class ClampWithGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, min, max):\n",
    "        ctx.min = min\n",
    "        ctx.max = max\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min, max)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
    "\n",
    "clamp_with_grad = ClampWithGrad.apply\n",
    "\n",
    "\n",
    "def vector_quantize(x, codebook):\n",
    "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
    "    indices = d.argmin(-1)\n",
    "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
    "    return replace_grad(x_q, x)\n",
    "\n",
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow # not used with pooling\n",
    "        \n",
    "        # Pick your own augments & their order\n",
    "        augment_list = []\n",
    "        for item in args.augments[0]:\n",
    "            if item == 'Ji':\n",
    "                augment_list.append(K.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1, p=0.7))\n",
    "            elif item == 'Sh':\n",
    "                augment_list.append(K.RandomSharpness(sharpness=0.3, p=0.5))\n",
    "            elif item == 'Gn':\n",
    "                augment_list.append(K.RandomGaussianNoise(mean=0.0, std=1., p=0.5))\n",
    "            elif item == 'Pe':\n",
    "                augment_list.append(K.RandomPerspective(distortion_scale=0.7, p=0.7))\n",
    "            elif item == 'Ro':\n",
    "                augment_list.append(K.RandomRotation(degrees=15, p=0.7))\n",
    "            elif item == 'Af':\n",
    "                augment_list.append(K.RandomAffine(degrees=15, translate=0.1, shear=5, p=0.7, padding_mode='zeros', keepdim=True)) # border, reflection, zeros\n",
    "            elif item == 'Et':\n",
    "                augment_list.append(K.RandomElasticTransform(p=0.7))\n",
    "            elif item == 'Ts':\n",
    "                augment_list.append(K.RandomThinPlateSpline(scale=0.8, same_on_batch=True, p=0.7))\n",
    "            elif item == 'Cr':\n",
    "                augment_list.append(K.RandomCrop(size=(self.cut_size,self.cut_size), pad_if_needed=True, padding_mode='reflect', p=0.5))\n",
    "            elif item == 'Er':\n",
    "                augment_list.append(K.RandomErasing(scale=(.1, .4), ratio=(.3, 1/.3), same_on_batch=True, p=0.7))\n",
    "            elif item == 'Re':\n",
    "                augment_list.append(K.RandomResizedCrop(size=(self.cut_size,self.cut_size), scale=(0.1,1),  ratio=(0.75,1.333), cropping_mode='resample', p=0.5))\n",
    "                \n",
    "        self.augs = nn.Sequential(*augment_list)\n",
    "        self.noise_fac = 0.1\n",
    "        # self.noise_fac = False\n",
    "\n",
    "        # Uncomment if you like seeing the list ;)\n",
    "        # print(augment_list)\n",
    "        \n",
    "        # Pooling\n",
    "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        cutouts = []\n",
    "        \n",
    "        for _ in range(self.cutn):            \n",
    "            # Use Pooling\n",
    "            cutout = (self.av_pool(input) + self.max_pool(input))/2\n",
    "            cutouts.append(cutout)\n",
    "            \n",
    "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
    "        \n",
    "        if self.noise_fac:\n",
    "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
    "            batch = batch + facs * torch.randn_like(batch)\n",
    "        return batch\n",
    "\n",
    "\n",
    "# An updated version with Kornia augments and pooling (where my version started):\n",
    "class MakeCutoutsPoolingUpdate(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow # Not used with pooling\n",
    "\n",
    "        self.augs = nn.Sequential(\n",
    "            K.RandomAffine(degrees=15, translate=0.1, p=0.7, padding_mode='border'),\n",
    "            K.RandomPerspective(0.7,p=0.7),\n",
    "            K.ColorJitter(hue=0.1, saturation=0.1, p=0.7),\n",
    "            K.RandomErasing((.1, .4), (.3, 1/.3), same_on_batch=True, p=0.7),            \n",
    "        )\n",
    "        \n",
    "        self.noise_fac = 0.1\n",
    "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        \n",
    "        for _ in range(self.cutn):\n",
    "            cutout = (self.av_pool(input) + self.max_pool(input))/2\n",
    "            cutouts.append(cutout)\n",
    "            \n",
    "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
    "        \n",
    "        if self.noise_fac:\n",
    "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
    "            batch = batch + facs * torch.randn_like(batch)\n",
    "        return batch\n",
    "\n",
    "\n",
    "# An Nerdy updated version with selectable Kornia augments, but no pooling:\n",
    "class MakeCutoutsNRUpdate(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "        self.noise_fac = 0.1\n",
    "        \n",
    "        # Pick your own augments & their order\n",
    "        augment_list = []\n",
    "        for item in args.augments[0]:\n",
    "            if item == 'Ji':\n",
    "                augment_list.append(K.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1, p=0.7))\n",
    "            elif item == 'Sh':\n",
    "                augment_list.append(K.RandomSharpness(sharpness=0.3, p=0.5))\n",
    "            elif item == 'Gn':\n",
    "                augment_list.append(K.RandomGaussianNoise(mean=0.0, std=1., p=0.5))\n",
    "            elif item == 'Pe':\n",
    "                augment_list.append(K.RandomPerspective(distortion_scale=0.5, p=0.7))\n",
    "            elif item == 'Ro':\n",
    "                augment_list.append(K.RandomRotation(degrees=15, p=0.7))\n",
    "            elif item == 'Af':\n",
    "                augment_list.append(K.RandomAffine(degrees=30, translate=0.1, shear=5, p=0.7, padding_mode='zeros', keepdim=True)) # border, reflection, zeros\n",
    "            elif item == 'Et':\n",
    "                augment_list.append(K.RandomElasticTransform(p=0.7))\n",
    "            elif item == 'Ts':\n",
    "                augment_list.append(K.RandomThinPlateSpline(scale=0.8, same_on_batch=True, p=0.7))\n",
    "            elif item == 'Cr':\n",
    "                augment_list.append(K.RandomCrop(size=(self.cut_size,self.cut_size), pad_if_needed=True, padding_mode='reflect', p=0.5))\n",
    "            elif item == 'Er':\n",
    "                augment_list.append(K.RandomErasing(scale=(.1, .4), ratio=(.3, 1/.3), same_on_batch=True, p=0.7))\n",
    "            elif item == 'Re':\n",
    "                augment_list.append(K.RandomResizedCrop(size=(self.cut_size,self.cut_size), scale=(0.1,1),  ratio=(0.75,1.333), cropping_mode='resample', p=0.5))\n",
    "                \n",
    "        self.augs = nn.Sequential(*augment_list)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
    "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
    "        if self.noise_fac:\n",
    "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
    "            batch = batch + facs * torch.randn_like(batch)\n",
    "        return batch\n",
    "\n",
    "\n",
    "# An updated version with Kornia augments, but no pooling:\n",
    "class MakeCutoutsUpdate(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "        self.augs = nn.Sequential(\n",
    "            K.RandomHorizontalFlip(p=0.5),\n",
    "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n",
    "            # K.RandomSolarize(0.01, 0.01, p=0.7),\n",
    "            K.RandomSharpness(0.3,p=0.4),\n",
    "            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n",
    "            K.RandomPerspective(0.2,p=0.4),)\n",
    "        self.noise_fac = 0.1\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
    "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
    "        if self.noise_fac:\n",
    "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
    "            batch = batch + facs * torch.randn_like(batch)\n",
    "        return batch\n",
    "\n",
    "\n",
    "# This is the original version (No pooling)\n",
    "class MakeCutoutsOrig(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
    "        return clamp_with_grad(torch.cat(cutouts, dim=0), 0, 1)\n",
    "\n",
    "\n",
    "def load_vqgan_model(config_path, checkpoint_path):\n",
    "    global gumbel\n",
    "    gumbel = False\n",
    "    config = OmegaConf.load(config_path)\n",
    "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
    "        model = vqgan.VQModel(**config.model.params)\n",
    "        model.eval().requires_grad_(False)\n",
    "        model.init_from_ckpt(checkpoint_path)\n",
    "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
    "        model = vqgan.GumbelVQ(**config.model.params)\n",
    "        model.eval().requires_grad_(False)\n",
    "        model.init_from_ckpt(checkpoint_path)\n",
    "        gumbel = True\n",
    "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
    "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
    "        parent_model.eval().requires_grad_(False)\n",
    "        parent_model.init_from_ckpt(checkpoint_path)\n",
    "        model = parent_model.first_stage_model\n",
    "    else:\n",
    "        raise ValueError(f'unknown model type: {config.model.target}')\n",
    "    del model.loss\n",
    "    return model\n",
    "\n",
    "\n",
    "def resize_image(image, out_size):\n",
    "    ratio = image.size[0] / image.size[1]\n",
    "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
    "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
    "    return image.resize(size, Image.LANCZOS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5be3c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "Restored from checkpoints/vqgan_imagenet_f16_16384.ckpt\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "vqgan = load_vqgan_model('checkpoints/vqgan_imagenet_f16_16384.yaml', 'checkpoints/vqgan_imagenet_f16_16384.ckpt').to(device)\n",
    "jit = True if float(torch.__version__[:3]) < 1.8 else False\n",
    "#perceptor = clip.load('ViT-B/32', jit=jit)[0].eval().requires_grad_(False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4f7f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bcb9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_quantize(x, codebook):\n",
    "    #Squares and sums along the last dimension, adds codebook squared and summed \n",
    "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
    "    indices = d.argmin(-1)\n",
    "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
    "    return replace_grad(x_q, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ace2a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z.movedim(1, 3) @ model.quantize.embedding.weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f829737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PIL\n",
    "\n",
    "# PIL.Image.fromarray(((result* 255).squeeze(0).permute(2,1,0).cpu().detach().numpy()).astype(np.uint8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dad6dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z.requires_grad_(True)\n",
    "# model.quantize.embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc7c3ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folder(folder_name):\n",
    "    \n",
    "    \n",
    "    folder_files = os.listdir(folder_name)\n",
    "    folder_tensors = torch.vstack([preprocess(Image.open(folder_name + folder_file)).unsqueeze(0).to(device) for folder_file in folder_files])\n",
    "    return folder_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b5796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = torchvision.transforms.Compose(\n",
    "   [Resize(size=224),\n",
    "    CenterCrop(size=(224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff8d480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_a = process_folder(\"data/tyrus/\")\n",
    "artist_b = process_folder(\"data/rothko/\")\n",
    "artist_c = process_folder(\"data/joseon/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d82254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#between 0 and 1\n",
    "def normalize(vector, zero_center=True):\n",
    "    zero_center_vector =  vector - vector.min()\n",
    "    change_vector_spread = zero_center_vector / (zero_center_vector.max() - zero_center_vector.min())\n",
    "    if not zero_center:\n",
    "        return change_vector_spread\n",
    "    else:\n",
    "        return  (change_vector_spread * 2) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6000ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_batch(artist_samples):\n",
    "    return torch.stack([normalize(artist_sample) for artist_sample in artist_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cc8d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"data/tyrus/\", \"data/rothko/\",\"data/joseon/\"]\n",
    "\n",
    "sizes = [len(os.listdir(data_path)) for data_path in folders]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17a09b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_images = [process_folder(data_path) for data_path in folders]\n",
    "normalized_images = torch.vstack([normalize_batch(artist_sample) for artist_sample in processed_images])\n",
    "#encoded_z, *_ = vqgan.encode((normalized_images).to(device))\n",
    "\n",
    "#vq_z = vector_quantize(encoded_z.movedim(1, 3), vqgan.quantize.embedding.weight).movedim(3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3eab8",
   "metadata": {},
   "source": [
    "import umap\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "flatten_vq_z = vq_z.reshape(vq_z.shape[0],-1).cpu().detach().numpy()\n",
    "standard_embedding = umap.UMAP(random_state=23).fit_transform(flatten_vq_z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac341366",
   "metadata": {},
   "source": [
    "c = ['r'] * sizes[0] + ['b'] * sizes[1] + ['g'] * sizes[2]\n",
    "plt.scatter(standard_embedding[:, 0], standard_embedding[:, 1], c=c, s=10, cmap='Spectral');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae1fd6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_artist_a = torch.stack([normalize(artist_sample) for artist_sample in artist_a])\n",
    "# normalized_artist_b = torch.stack([normalize(artist_sample) for artist_sample in artist_b])\n",
    "# normalized_artist_b = torch.stack([normalize(artist_sample) for artist_sample in artist_b])\n",
    "\n",
    "# artista_z, *_ = vqgan.encode(normalized_artist_a.to(device))\n",
    "# artistb_z, *_ = vqgan.encode(normalized_artist_b.to(device))\n",
    "\n",
    "# artista_z_q = vector_quantize(artista_z.movedim(1, 3), vqgan.quantize.embedding.weight).movedim(3, 1)\n",
    "# artistb_z_q = vector_quantize(artistb_z.movedim(1, 3), vqgan.quantize.embedding.weight).movedim(3, 1)\n",
    "\n",
    "# flattened_artista_z_q = artista_z_q.reshape(artista_z_q.shape[0],-1)\n",
    "# flattened_artistb_z_q = artistb_z_q.reshape(artistb_z_q.shape[0],-1)\n",
    "\n",
    "# artista_meandiff = flattened_artista_z_q.mean(axis=0)\n",
    "# artistb_meandiff = flattened_artistb_z_q.mean(axis=0)\n",
    "\n",
    "# artist_latent_codes = torch.vstack([flattened_artista_z_q, flattened_artistb_z_q]).cpu().detach().numpy()\n",
    "# standard_embedding = umap.UMAP(random_state=23).fit_transform(artist_latent_codes)\n",
    "\n",
    "# # flattened_artistb_z_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6ed72191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = random_gradient_image(224,224)\n",
    "# pil_image = img.convert('RGB')\n",
    "# pil_image = pil_image.resize((256, 256), Image.LANCZOS)\n",
    "# pil_tensor = TF.to_tensor(pil_image)\n",
    "# z, *_ = vqgan.encode(pil_tensor.to(device).unsqueeze(0) * 2 - 1)\n",
    "# z_q = vector_quantize(z.movedim(1, 3), vqgan.quantize.embedding.weight).movedim(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "18d29659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in vqgan.encoder.named_modules():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b96311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assist from https://web.stanford.edu/~nanbhas/blog/forward-hooks-pytorch/\n",
    "activations = {}\n",
    "gradients = {}\n",
    "def getActivation(name):\n",
    "    # the hook signature \n",
    "    def hook(model, input, output):\n",
    "        \n",
    "        output.requires_grad_(True)\n",
    "        output.retain_grad()\n",
    "        gradients[name] = output.grad\n",
    "        activations[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c86ba4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqgan_down_blocks = [down_block.block[0] for down_block in vqgan.encoder.down]\n",
    "vqgan_mid_blocks = [vqgan.encoder.mid.block_1, vqgan.encoder.mid.block_2]\n",
    "vqgan_blocks = vqgan_down_blocks[-1:] + vqgan_mid_blocks[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8a463b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks = []\n",
    "layers = vqgan_blocks\n",
    "layernames = np.concatenate([[f'ResNetBlock{i}' for i in range(0,len(vqgan_blocks))]], dtype=str)\n",
    "for l, n in zip(layers, layernames):\n",
    "    hooks.append(l.register_forward_hook(getActivation(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cebc94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook:\n",
    "    \"\"\"Attaches to a module and records its activations and gradients.\"\"\"\n",
    "\n",
    "    def __init__(self, module: nn.Module):\n",
    "        self.data = None\n",
    "        self.hook = module.register_forward_hook(self.save_grad)\n",
    "        \n",
    "    def save_grad(self, module, input, output):\n",
    "        self.data = output\n",
    "        output.requires_grad_(True)\n",
    "        output.retain_grad()\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        self.hook.remove()\n",
    "        \n",
    "    @property\n",
    "    def activation(self) -> torch.Tensor:\n",
    "        return self.data\n",
    "    \n",
    "#     @property\n",
    "#     def gradient(self) -> torch.Tensor:\n",
    "#         return self.data.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "febf4713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_activations_for_image(image_tensor):\n",
    "    all_layer_activations = {}\n",
    "    for layer, name in zip(layers, layernames):\n",
    "        #layer_gradients = []\n",
    "        layer_activations = []\n",
    "        with Hook(layer) as hook:\n",
    "\n",
    "            # Do a forward and backward pass.\n",
    "            with torch.no_grad():\n",
    "                output,*_ = vqgan.encode((image_tensor).to(device))\n",
    "            #output.backward(text_tensor.unsqueeze(0))\n",
    "\n",
    "            #grad = hook.gradient.float()\n",
    "            act = hook.activation.float()\n",
    "            #layer_gradients.append(grad)\n",
    "            layer_activations.append(act)\n",
    "        #all_layer_gradients[name] = layer_gradients\n",
    "        all_layer_activations[name] = layer_activations\n",
    "    return all_layer_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0906958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 3, 224, 224])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad93fb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_layer_activations = record_activations_for_image(normalized_images[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c3dfd37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ResNetBlock0', 'ResNetBlock1'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_layer_activations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3cf937b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 512, 14, 14])\n",
      "torch.Size([30, 512, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "for key in all_layer_activations.keys():\n",
    "    print(all_layer_activations[key][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b4dc2bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(all_layer_activations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e90b03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, svd_solver='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3bde1bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "transformed_data = all_layer_activations[keys[1]][0].reshape(30,-1).cpu().detach().numpy()\n",
    "transformed_data = scaler.fit_transform(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a2d7a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data = pca.fit_transform(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b9ac7310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "40340356",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_a = process_folder(\"data/tyrus/\")\n",
    "artist_b = process_folder(\"data/rothko/\")\n",
    "artist_c = process_folder(\"data/joseon/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c78a48e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = ['r'] * sizes[0] + ['b'] * sizes[1] + ['g'] * sizes[2]\n",
    "\n",
    "plt.scatter(pca_data[:, 0], pca_data[:, 1], c=c, s=10, cmap='Spectral');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "16600449",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data = pca.fit_transform(scaler.transform(transformed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "73d9256d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.1678375e+02,  1.3698669e-01],\n",
       "       [-3.1650909e+02,  1.3710558e-01]], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4148a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
