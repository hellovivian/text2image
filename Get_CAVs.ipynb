{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac679899",
   "metadata": {},
   "source": [
    "# Notebook Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f06f1",
   "metadata": {},
   "source": [
    "Implement TCAV using Pytorch for CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a8b248",
   "metadata": {},
   "source": [
    "# Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32277860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/openai/CLIP\n",
    "# authors Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings), nerdyrodent\n",
    "# authors vivian\n",
    "# The original BigGAN+CLIP method was by https://twitter.com/advadnoun\n",
    "import threading\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import math\n",
    "import random\n",
    "from urllib.request import urlopen\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('taming-transformers')\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models import cond_transformer, vqgan\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.cuda import get_device_properties\n",
    "torch.backends.cudnn.benchmark = False\n",
    "from torch_optimizer import DiffGrad, AdamP, RAdam\n",
    "from CLIP import clip\n",
    "import kornia.augmentation as K\n",
    "import imageio\n",
    "from PIL import ImageFile, Image, PngImagePlugin, ImageChops\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from subprocess import Popen, PIPE\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3033a3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B/32', 'ViT-B/16']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6fad75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3410ac22",
   "metadata": {},
   "source": [
    "# Load CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e379417",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load('ViT-B/32', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d901147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook:\n",
    "    \"\"\"Attaches to a module and records its activations and gradients.\"\"\"\n",
    "\n",
    "    def __init__(self, module: nn.Module):\n",
    "        self.data = None\n",
    "        self.hook = module.register_forward_hook(self.save_grad)\n",
    "        \n",
    "    def save_grad(self, module, input, output):\n",
    "        self.data = output\n",
    "        output.requires_grad_(True)\n",
    "        output.retain_grad()\n",
    "        \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        self.hook.remove()\n",
    "        \n",
    "    @property\n",
    "    def activation(self) -> torch.Tensor:\n",
    "        return self.data\n",
    "    \n",
    "    @property\n",
    "    def gradient(self) -> torch.Tensor:\n",
    "        return self.data.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da4b2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            # reset counter if validation loss improves\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping')\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1cac60",
   "metadata": {},
   "source": [
    "# Register hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57a45a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assist from https://web.stanford.edu/~nanbhas/blog/forward-hooks-pytorch/\n",
    "activations = {}\n",
    "gradients = {}\n",
    "def getActivation(name):\n",
    "    # the hook signature \n",
    "    def hook(model, input, output):\n",
    "        \n",
    "        output.requires_grad_(True)\n",
    "        output.retain_grad()\n",
    "        gradients[name] = output.grad\n",
    "        activations[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "971a39aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooks = []\n",
    "layers = np.concatenate([[model.visual.conv1], model.visual.transformer.resblocks[1::2]])\n",
    "layernames = np.concatenate([['layer0'], [f'layer{i}' for i in range(1,13,2)]], dtype=str)\n",
    "for l, n in zip(layers, layernames):\n",
    "    hooks.append(l.register_forward_hook(getActivation(n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa86e9",
   "metadata": {},
   "source": [
    "# Image Encoding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1f0b5dc",
   "metadata": {},
   "source": [
    "image = preprocess(Image.open(\"square.jpg\")).unsqueeze(0).to(device)\n",
    "image_features = model.encode_image(image.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0f015",
   "metadata": {},
   "source": [
    "# Text Encoding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad6e1007",
   "metadata": {},
   "source": [
    "prompt = \"square shaped cat\"\n",
    "txt, weight, stop = split_prompt(prompt)\n",
    "\n",
    "text_features = model.encode_text(clip.tokenize(txt).to(device)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cca0b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_tensors(img_filename, img_dir=\"\"):\n",
    "    image = preprocess(Image.open(img_dir + img_filename)).unsqueeze(0).to(device)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0045a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_images(img_filename, img_dir=\"\"):\n",
    "    image = preprocess(Image.open(img_dir + img_filename)).unsqueeze(0).to(device)\n",
    "\n",
    "    image_features = model.encode_image(image.cuda())\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4b54b",
   "metadata": {},
   "source": [
    "Load an example image"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5747d983",
   "metadata": {},
   "source": [
    "# PIL.Image.open\n",
    "# concept_filenames[0]\n",
    "\n",
    "Image.open('tcav/concepts/striped/striped_0086.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a5736",
   "metadata": {},
   "source": [
    "# Define Linear Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a126a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(num_features, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_x):\n",
    "        x = self.linear1(input_x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "188a0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61c0aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifiers_sets = np.load(\"classifiers_perclass_perlayer_smeared_dotted_knitted_spiralled_chequered.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2a4c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orthogonal_vector(classifier, classifier_size):\n",
    "    weight, bias = [param for param in classifier.parameters()]\n",
    "    weight_vector = weight.squeeze().cpu().detach().numpy()\n",
    "    orthonormal_vector = np.random.randn(classifier_size)  # take a random vector\n",
    "    orthonormal_vector -= orthonormal_vector.dot(weight_vector) * weight_vector / np.linalg.norm(weight_vector)**2\n",
    "    orthonormal_vector /= np.linalg.norm(orthonormal_vector) \n",
    "    return orthonormal_vector, weight_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cebafa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "cavs = {}\n",
    "\n",
    "concepts = [\"smeared\", \"dotted\", \"knitted\", \"spiralled\", \"chequered\"]\n",
    "\n",
    "for linear_classifier_set, concept in zip(linear_classifiers_sets, concepts):\n",
    "    orthonormal_vector, weight_vector = get_orthogonal_vector(linear_classifier_set[-1], linear_classifier_set[-1].linear1.in_features)\n",
    "    cavs[concept] = orthonormal_vector\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a8b2abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_classifiers[0][-1].linear1.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96651f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"concept_cavs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cavs,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1476fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (replearn2)",
   "language": "python",
   "name": "replearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
