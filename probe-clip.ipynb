{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32277860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/openai/CLIP\n",
    "\n",
    "import threading\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.colors import LinearSegmentedColormap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fca8342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally made by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings)\n",
    "# The original BigGAN+CLIP method was by https://twitter.com/advadnoun\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "# from email.policy import default\n",
    "from urllib.request import urlopen\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('taming-transformers')\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models import cond_transformer, vqgan\n",
    "#import taming.modules \n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.cuda import get_device_properties\n",
    "torch.backends.cudnn.benchmark = False\t\t# NR: True is a bit faster, but can lead to OOM. False is more deterministic.\n",
    "#torch.use_deterministic_algorithms(True)\t# NR: grid_sampler_2d_backward_cuda does not have a deterministic implementation\n",
    "\n",
    "from torch_optimizer import DiffGrad, AdamP, RAdam\n",
    "\n",
    "from CLIP import clip\n",
    "import kornia.augmentation as K\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "from PIL import ImageFile, Image, PngImagePlugin, ImageChops\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "import re\n",
    "\n",
    "# Check for GPU and reduce the default image size if low VRAM\n",
    "default_image_size = 256  # >8GB VRAM\n",
    "\n",
    "class Prompt(nn.Module):\n",
    "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
    "        super().__init__()\n",
    "        self.register_buffer('embed', embed)\n",
    "        self.register_buffer('weight', torch.as_tensor(weight))\n",
    "        self.register_buffer('stop', torch.as_tensor(stop))\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
    "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
    "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
    "        dists = dists * self.weight.sign()\n",
    "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
    "\n",
    "#NR: Split prompts and weights\n",
    "def split_prompt(prompt):\n",
    "    vals = prompt.rsplit(':', 2)\n",
    "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
    "    return vals[0], float(vals[1]), float(vals[2])\n",
    "\n",
    "def resize_image(image, out_size):\n",
    "    ratio = image.size[0] / image.size[1]\n",
    "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
    "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
    "    return image.resize(size, Image.LANCZOS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b27cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.concept import TCAV\n",
    "from captum.attr import (\n",
    "    IntegratedGradients,\n",
    "    LayerIntegratedGradients,\n",
    "    TokenReferenceBase,\n",
    "    configure_interpretable_embedding_layer,\n",
    "    remove_interpretable_embedding_layer,\n",
    "    visualization\n",
    ")\n",
    "from captum.attr._utils.input_layer_wrapper import ModelInputWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6fad75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3410ac22",
   "metadata": {},
   "source": [
    "# Load CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e379417",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load('ViT-B/32', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa86e9",
   "metadata": {},
   "source": [
    "# Image Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73c14e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = preprocess(Image.open(\"square.jpg\")).unsqueeze(0).to(device)\n",
    "# image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "image_features = model.encode_image(image.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0f015",
   "metadata": {},
   "source": [
    "# Text Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c555ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"square shaped cat\"\n",
    "txt, weight, stop = split_prompt(prompt)\n",
    "\n",
    "text_features = model.encode_text(clip.tokenize(txt).to(device)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c596b",
   "metadata": {},
   "source": [
    "# Running Inference from CLIP (from CLIP repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2736da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = IntegratedGradients(model)\n",
    "\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "interpretable_embedding = configure_interpretable_embedding_layer = model.token_embedding\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    inputs = (image_features, text_features)\n",
    "    baselines = (image_features * 0.0, text_features * 0.0)\n",
    "\n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    \n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "    probs_max = probs.max()\n",
    "    progs_argmax = probs.argmax()\n",
    "\n",
    "#     attributions = attr.attribute(inputs=(image, text),\n",
    "#                                     baselines=(image * 0, text * 0),\n",
    "#                                     target=progs_argmax,\n",
    "#                                     n_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e1adb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = (image_features, text_features)\n",
    "# baselines = (image_features * 0.0, text_features * 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f4be33",
   "metadata": {},
   "source": [
    "# Code that we shall need to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf774a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=['inception4c', 'inception4d', 'inception4e']\n",
    "\n",
    "tcav = TCAV(model=model,\n",
    "              layers=layers,\n",
    "              layer_attr_method = LayerIntegratedGradients(\n",
    "                model, None, multiply_by_inputs=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
