{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2366477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import resnet  # from pytorch-resnet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# from model import Net, apply_attention, tile_2d_over_nd # from pytorch-vqa\n",
    "# from utils import get_transform # from pytorch-vqa\n",
    "\n",
    "from captum.attr import (\n",
    "    IntegratedGradients,\n",
    "    LayerIntegratedGradients,\n",
    "    TokenReferenceBase,\n",
    "    configure_interpretable_embedding_layer,\n",
    "    remove_interpretable_embedding_layer,\n",
    "    visualization\n",
    ")\n",
    "from captum.attr._utils.input_layer_wrapper import ModelInputWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb97caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally made by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings)\n",
    "# The original BigGAN+CLIP method was by https://twitter.com/advadnoun\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "# from email.policy import default\n",
    "from urllib.request import urlopen\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('taming-transformers')\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models import cond_transformer, vqgan\n",
    "#import taming.modules \n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.cuda import get_device_properties\n",
    "torch.backends.cudnn.benchmark = False\t\t# NR: True is a bit faster, but can lead to OOM. False is more deterministic.\n",
    "#torch.use_deterministic_algorithms(True)\t# NR: grid_sampler_2d_backward_cuda does not have a deterministic implementation\n",
    "\n",
    "from torch_optimizer import DiffGrad, AdamP, RAdam\n",
    "\n",
    "from CLIP import clip\n",
    "import kornia.augmentation as K\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "from PIL import ImageFile, Image, PngImagePlugin, ImageChops\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "import re\n",
    "\n",
    "# Check for GPU and reduce the default image size if low VRAM\n",
    "default_image_size = 256  # >8GB VRAM\n",
    "\n",
    "prompt = \"A cute, smiling, Nerdy Rodent\"\n",
    "\n",
    "# Various functions and classes\n",
    "def sinc(x):\n",
    "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
    "\n",
    "\n",
    "def lanczos(x, a):\n",
    "    cond = torch.logical_and(-a < x, x < a)\n",
    "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
    "    return out / out.sum()\n",
    "\n",
    "\n",
    "def ramp(ratio, width):\n",
    "    n = math.ceil(width / ratio + 1)\n",
    "    out = torch.empty([n])\n",
    "    cur = 0\n",
    "    for i in range(out.shape[0]):\n",
    "        out[i] = cur\n",
    "        cur += ratio\n",
    "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
    "\n",
    "# create initial gradient image\n",
    "def gradient_2d(start, stop, width, height, is_horizontal):\n",
    "    if is_horizontal:\n",
    "        return np.tile(np.linspace(start, stop, width), (height, 1))\n",
    "    else:\n",
    "        return np.tile(np.linspace(start, stop, height), (width, 1)).T\n",
    "\n",
    "\n",
    "def gradient_3d(width, height, start_list, stop_list, is_horizontal_list):\n",
    "    result = np.zeros((height, width, len(start_list)), dtype=float)\n",
    "\n",
    "    for i, (start, stop, is_horizontal) in enumerate(zip(start_list, stop_list, is_horizontal_list)):\n",
    "        result[:, :, i] = gradient_2d(start, stop, width, height, is_horizontal)\n",
    "\n",
    "    return result\n",
    "\n",
    "class ReplaceGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x_forward, x_backward):\n",
    "        ctx.shape = x_backward.shape\n",
    "        return x_forward\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        return None, grad_in.sum_to_size(ctx.shape)\n",
    "\n",
    "replace_grad = ReplaceGrad.apply\n",
    "\n",
    "class ClampWithGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, min, max):\n",
    "        ctx.min = min\n",
    "        ctx.max = max\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min, max)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
    "\n",
    "clamp_with_grad = ClampWithGrad.apply\n",
    "\n",
    "def vector_quantize(x, codebook):\n",
    "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
    "    indices = d.argmin(-1)\n",
    "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
    "    return replace_grad(x_q, x)\n",
    "\n",
    "class Prompt(nn.Module):\n",
    "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
    "        super().__init__()\n",
    "        self.register_buffer('embed', embed)\n",
    "        self.register_buffer('weight', torch.as_tensor(weight))\n",
    "        self.register_buffer('stop', torch.as_tensor(stop))\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
    "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
    "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
    "        dists = dists * self.weight.sign()\n",
    "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
    "\n",
    "#NR: Split prompts and weights\n",
    "def split_prompt(prompt):\n",
    "    vals = prompt.rsplit(':', 2)\n",
    "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
    "    return vals[0], float(vals[1]), float(vals[2])\n",
    "\n",
    "\n",
    "def load_vqgan_model(config_path, checkpoint_path):\n",
    "    config = OmegaConf.load(config_path)\n",
    "    config.model.target == 'taming.models.vqgan.VQModel'\n",
    "    model = vqgan.VQModel(**config.model.params)\n",
    "    model.eval().requires_grad_(False)\n",
    "    model.init_from_ckpt(checkpoint_path)\n",
    "    del model.loss\n",
    "    return model\n",
    "\n",
    "\n",
    "def resize_image(image, out_size):\n",
    "    ratio = image.size[0] / image.size[1]\n",
    "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
    "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
    "    return image.resize(size, Image.LANCZOS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f8fb863",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9eecbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "Restored from checkpoints/vqgan_imagenet_f16_16384.ckpt\n"
     ]
    }
   ],
   "source": [
    "model = load_vqgan_model(\"checkpoints/vqgan_imagenet_f16_16384.yaml\", \"checkpoints/vqgan_imagenet_f16_16384.ckpt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6801ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "jit = True if float(torch.__version__[:3]) < 1.8 else False\n",
    "perceptor = clip.load(\"ViT-B/32\", jit=jit)[0].eval().requires_grad_(False).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0531dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_size = perceptor.visual.input_resolution\n",
    "f = 2**(model.decoder.num_resolutions - 1)\n",
    "toksX, toksY = 256 // f,256 // f\n",
    "sideX, sideY = toksX * f, toksY * f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba29d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_dim = model.quantize.e_dim\n",
    "n_toks = model.quantize.n_e\n",
    "z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
    "z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
    "\n",
    "one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
    "\n",
    "z = one_hot @ model.quantize.embedding.weight\n",
    "\n",
    "z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb1d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow # not used with pooling\n",
    "        \n",
    "        # Pick your own augments & their order\n",
    "        augment_list = []\n",
    "        \n",
    "        self.augs = nn.Sequential(*augment_list)\n",
    "        self.noise_fac = 0.1\n",
    "\n",
    "        # Pooling\n",
    "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        cutouts = []\n",
    "        \n",
    "        for _ in range(self.cutn):            \n",
    "            # Use Pooling\n",
    "            cutout = (self.av_pool(input) + self.max_pool(input))/2\n",
    "            cutouts.append(cutout)\n",
    "            \n",
    "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
    "        \n",
    "        if self.noise_fac:\n",
    "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
    "            batch = batch + facs * torch.randn_like(batch)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b8d709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_size = perceptor.visual.input_resolution\n",
    "make_cutouts = MakeCutouts(cut_size, 32, cut_pow=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac58f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "z_orig = z.clone()\n",
    "z.requires_grad_(True)\n",
    "\n",
    "pMs = []\n",
    "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                  std=[0.26862954, 0.26130258, 0.27577711])\n",
    "\n",
    "prompt = \"dog in the style of da vinci\"\n",
    "txt, weight, stop = split_prompt(prompt)\n",
    "\n",
    "#okay so this is the embedding\n",
    "embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
    "pMs.append(Prompt(embed, weight, stop).to(device))\n",
    "\n",
    "\n",
    "opt = optim.Adam([z], lr=0.1)\t# LR=0.1 (Default)\n",
    " \n",
    "# Output for the user\n",
    "print('Using device:', device)\n",
    "print('Optimising using:', opt)\n",
    "\n",
    "seed = 16\n",
    "torch.manual_seed(seed)\n",
    "print('Using seed:', seed)\n",
    "\n",
    "\n",
    "# Vector quantize\n",
    "def synth(z):\n",
    "    #model.quantize.embedding.weight is the codebook\n",
    "    z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
    "    img = model.decode(z_q).add(1).div(2)\n",
    "\n",
    "    return clamp_with_grad(img, 0, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def checkin(i, losses):\n",
    "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
    "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
    "    out = synth(z)\n",
    "    info = PngImagePlugin.PngInfo()\n",
    "    info.add_text('comment', f'{prompt}')\n",
    "    TF.to_pil_image(out[0].cpu()).save(\"test.png\", pnginfo=info)\n",
    "\n",
    "\n",
    "def ascend_txt():\n",
    "    global i\n",
    "    out = synth(z)\n",
    "    iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    if 0.:\n",
    "        # result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
    "        result.append(F.mse_loss(z, torch.zeros_like(z_orig)) * ((1/torch.tensor(i*2 + 1))*0) / 2)\n",
    "\n",
    "        \n",
    "    for prompt in pMs:\n",
    "        print(iii.shape)\n",
    "        result.append(prompt(iii))\n",
    "    \n",
    "    return result # return loss\n",
    "\n",
    "\n",
    "def train(i):\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    lossAll = ascend_txt()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        checkin(i, lossAll)\n",
    "       \n",
    "    loss = sum(lossAll)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z.copy_(z.maximum(z_min).minimum(z_max))\n",
    "\n",
    "for i in range(200):         \n",
    "\n",
    "    train(i)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea167f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
